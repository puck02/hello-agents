{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dd64c0",
   "metadata": {},
   "source": [
    "# LLM Client (å¤§æ¨¡å‹å®¢æˆ·ç«¯) å®æˆ˜\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬æ¼”ç¤ºäº†å¦‚ä½•å°è£…ä¸€ä¸ªé€šç”¨çš„ LLM å®¢æˆ·ç«¯ï¼Œç”¨äºè°ƒç”¨å…¼å®¹ OpenAI æ¥å£çš„å¤§æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ GitHub Models, DeepSeek, OpenAI ç­‰ï¼‰ã€‚\n",
    "\n",
    "### æ ¸å¿ƒåŠŸèƒ½ï¼š\n",
    "1. **ç»Ÿä¸€æ¥å£**ï¼šé€šè¿‡ `HelloAgentsLLM` ç±»å°è£…å¤æ‚çš„è°ƒç”¨ç»†èŠ‚ã€‚\n",
    "2. **ç¯å¢ƒå˜é‡ç®¡ç†**ï¼šä½¿ç”¨ `.env` æ–‡ä»¶éš”ç¦»å¯†é’¥å®‰å…¨ã€‚\n",
    "3. **æµå¼è¾“å‡º**ï¼šå®æ—¶æ‰“å°æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚\n",
    "4. **C/C++ ç±»æ¯”**ï¼šå°† Python ç±»å’Œæ–¹æ³•ä¸ C++ æ¦‚å¿µå¯¹ç…§ç†è§£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b2248a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰é…ç½®çš„æ¨¡å‹: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1. å¯¼å…¥åº“ä¸ç¯å¢ƒé…ç½®\n",
    "# load_dotenv() ç±»æ¯” C++ çš„ LoadConfig() å‡½æ•°ï¼Œä»ç£ç›˜è¯»å–å˜é‡å¹¶æ³¨å…¥ç³»ç»Ÿç¯å¢ƒã€‚\n",
    "# é»˜è®¤ä¼šå¯»æ‰¾é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .env æ–‡ä»¶ã€‚\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "# æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦åŠ è½½æˆåŠŸ\n",
    "model_id = os.getenv(\"LLM_MODEL_ID\")\n",
    "print(f\"å½“å‰é…ç½®çš„æ¨¡å‹: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9240d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelloAgentsLLM:\n",
    "    \"\"\"\n",
    "    ä¸ºæœ¬ä¹¦ \"Hello Agents\" å®šåˆ¶çš„LLMå®¢æˆ·ç«¯ã€‚\n",
    "    ç±»æ¯” C++ çš„ Wrapper Classï¼Œå°†åº•å±‚çš„ OpenAI SDK å°è£…æˆé€‚åˆæœ¬é¡¹ç›®ä½¿ç”¨çš„æ¥å£ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):\n",
    "        \"\"\"\n",
    "        æ„é€ å‡½æ•°ã€‚\n",
    "        \"\"\"\n",
    "        # è¿™é‡Œçš„ or è¿ç®—é€»è¾‘ç±»æ¯” C++ çš„ä¸‰å…ƒè¿ç®—ç¬¦æˆ–é»˜è®¤å‚æ•°å¤„ç†ã€‚\n",
    "        self.model = model or os.getenv(\"LLM_MODEL_ID\")\n",
    "        apiKey = apiKey or os.getenv(\"LLM_API_KEY\")\n",
    "        baseUrl = baseUrl or os.getenv(\"LLM_BASE_URL\")\n",
    "        timeout = timeout or int(os.getenv(\"LLM_TIMEOUT\", 60))\n",
    "        \n",
    "        if not all([self.model, apiKey, baseUrl]):\n",
    "            raise ValueError(\"æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚\")\n",
    "\n",
    "        # å®ä¾‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œç±»æ¯” C++ çš„æ™ºèƒ½æŒ‡é’ˆæˆ– Resource Manager\n",
    "        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)\n",
    "\n",
    "    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:\n",
    "        \"\"\"\n",
    "        æ ¸å¿ƒæ€è€ƒæ–¹æ³•ã€‚\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...\")\n",
    "        try:\n",
    "            # stream=True å¼€å¯æµå¼å“åº”ï¼Œç±»æ¯” C++ çš„ Callback æˆ–å¼‚æ­¥æµã€‚\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            print(\"âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\")\n",
    "            collected_content = []\n",
    "            # è¿­ä»£ chunk ç±»æ¯” C++ çš„ while(stream.read())\n",
    "            for chunk in response:\n",
    "                if chunk.choices:\n",
    "                    content = chunk.choices[0].delta.content or \"\"\n",
    "                    print(content, end=\"\", flush=True) # flush=True ç¡®ä¿å­—ç¬¦ç«‹å³æ˜¾ç¤ºåœ¨å±å¹•ä¸Š\n",
    "                    collected_content.append(content)\n",
    "            print() \n",
    "            return \"\".join(collected_content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "696a0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- è°ƒç”¨LLM ---\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o-mini æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "å¿«é€Ÿæ’åºï¼ˆQuick Sortï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„æ’åºç®—æ³•ï¼Œé‡‡ç”¨åˆ†æ²»æ³•ï¼ˆDivide and Conquerï¼‰ç­–ç•¥ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç”¨ Python å®ç°çš„å¿«é€Ÿæ’åºç®—æ³•çš„ç¤ºä¾‹ä»£ç ï¼š\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]  # é€‰æ‹©ä¸­é—´çš„å…ƒç´ ä½œä¸ºåŸºå‡†\n",
      "        left = [x for x in arr if x < pivot]  # å°äºåŸºå‡†çš„å…ƒç´ \n",
      "        middle = [x for x in arr if x == pivot]  # ç­‰äºåŸºå‡†çš„å…ƒç´ \n",
      "        right = [x for x in arr if x > pivot]  # å¤§äºåŸºå‡†çš„å…ƒç´ \n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# ç¤ºä¾‹ç”¨æ³•\n",
      "if __name__ == \"__main__\":\n",
      "    arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "    sorted_arr = quick_sort(arr)\n",
      "    print(\"æ’åºåçš„æ•°ç»„:\", sorted_arr)\n",
      "```\n",
      "\n",
      "### ä»£ç è¯´æ˜ï¼š\n",
      "1. **åŸºå‡†é€‰æ‹©**ï¼šé€‰æ‹©æ•°ç»„ä¸­é—´çš„å…ƒç´ ä½œä¸ºåŸºå‡†ï¼ˆpivotï¼‰ã€‚\n",
      "2. **åˆ†åŒº**ï¼šå°†æ•°ç»„åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š\n",
      "   - å°äºåŸºå‡†çš„å…ƒç´ ï¼ˆleftï¼‰\n",
      "   - ç­‰äºåŸºå‡†çš„å…ƒç´ ï¼ˆmiddleï¼‰\n",
      "   - å¤§äºåŸºå‡†çš„å…ƒç´ ï¼ˆrightï¼‰\n",
      "3. **é€’å½’æ’åº**ï¼šå¯¹å°äºå’Œå¤§äºåŸºå‡†çš„éƒ¨åˆ†é€’å½’è°ƒç”¨ `quick_sort` å‡½æ•°ã€‚\n",
      "4. **åˆå¹¶ç»“æœ**ï¼šå°†æ’åºåçš„å·¦éƒ¨åˆ†ã€åŸºå‡†éƒ¨åˆ†å’Œå³éƒ¨åˆ†åˆå¹¶æˆä¸€ä¸ªæ–°çš„æ’åºæ•°ç»„ã€‚\n",
      "\n",
      "### ç¤ºä¾‹è¾“å‡ºï¼š\n",
      "è¿è¡Œä¸Šè¿°ä»£ç å°†è¾“å‡ºï¼š\n",
      "```\n",
      "æ’åºåçš„æ•°ç»„: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "ä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¾“å…¥æ•°ç»„ `arr` æ¥æµ‹è¯•ä¸åŒçš„æƒ…å†µã€‚\n",
      "\n",
      "--- å®Œæ•´æ¨¡å‹å“åº”é¢„è§ˆ ---\n",
      "ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦: 893\n"
     ]
    }
   ],
   "source": [
    "# 2. å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹\n",
    "# è¿™é‡Œæ¼”ç¤ºå¦‚ä½•å®ä¾‹åŒ–å¹¶è°ƒç”¨ï¼Œç±»æ¯” C++ çš„ main å‡½æ•°æµ‹è¯•éƒ¨åˆ†ã€‚\n",
    "\n",
    "try:\n",
    "    # è‡ªåŠ¨ä» .env è½½å…¥é…ç½®\n",
    "    llmClient = HelloAgentsLLM()\n",
    "    \n",
    "    # æ„é€ å¯¹è¯æ¶ˆæ¯ï¼Œç±»æ¯” Vector<Map<String, String>>\n",
    "    exampleMessages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes Python code.\"},\n",
    "        {\"role\": \"user\", \"content\": \"å†™ä¸€ä¸ªå¿«é€Ÿæ’åºç®—æ³•\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"--- è°ƒç”¨LLM ---\")\n",
    "    responseText = llmClient.think(exampleMessages)\n",
    "    \n",
    "    if responseText:\n",
    "        print(\"\\n--- å®Œæ•´æ¨¡å‹å“åº”é¢„è§ˆ ---\")\n",
    "        # æ‰“å°ç”Ÿæˆçš„å­—ç¬¦æ€»æ•°ï¼Œç±»æ¯” string.length()\n",
    "        print(f\"ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦: {len(responseText)}\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
