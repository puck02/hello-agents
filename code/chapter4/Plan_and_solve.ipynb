{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437d8782",
   "metadata": {},
   "source": [
    "# Plan-and-Solve æ™ºèƒ½ä½“å®æˆ˜\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬æ¼”ç¤ºäº†å¦å¤–ä¸€ç§ç»å…¸çš„æ™ºèƒ½ä½“èŒƒå¼ï¼š**Plan-and-Solve (å…ˆè§„åˆ’ï¼Œåæ‰§è¡Œ)**ã€‚\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "ä¸ ReAct çš„â€œèµ°ä¸€æ­¥çœ‹ä¸€æ­¥â€ä¸åŒï¼ŒPlan-and-Solve é‡‡ç”¨â€œä¸‰æ€è€Œåè¡Œâ€çš„ç­–ç•¥ï¼š\n",
    "1. **è§„åˆ’é˜¶æ®µ (Planner)**ï¼šå°†å¤æ‚é—®é¢˜ä¸€æ¬¡æ€§æ‹†è§£ä¸ºå¤šä¸ªé€»è¾‘è¿è´¯çš„å­ä»»åŠ¡ã€‚\n",
    "2. **æ‰§è¡Œé˜¶æ®µ (Executor)**ï¼šä¸¥æ ¼æŒ‰ç…§è®¡åˆ’é¡ºåºï¼Œé€ä¸€è§£å†³å­ä»»åŠ¡ï¼Œå¹¶è¿›è¡ŒçŠ¶æ€ç®¡ç†ã€‚\n",
    "\n",
    "### C/C++ ç±»æ¯”ï¼š\n",
    "è¿™å°±åƒæ˜¯ **ç¼–è¯‘å™¨çš„å·¥ä½œæµç¨‹**ï¼š\n",
    "- **Planner**ï¼šç±»æ¯”ä¸º **Frontend/Parser**ï¼Œå°†æºä»£ç ï¼ˆé—®é¢˜ï¼‰è§£æå¹¶ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼ˆIntermediate Representationï¼Œå³è®¡åˆ’åˆ—è¡¨ï¼‰ã€‚\n",
    "- **Executor**ï¼šç±»æ¯”ä¸º **Backend/Generator**ï¼Œæ ¹æ®ä¸­é—´è¡¨ç¤ºç”Ÿæˆæœºå™¨ç ï¼ˆæœ€ç»ˆç»“æœï¼‰ï¼Œæ¯ä¸€æ­¥ç”Ÿæˆéƒ½ä¾èµ–äºç¬¦å·è¡¨ï¼ˆå†å²çŠ¶æ€ï¼‰çš„æ›´æ–°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1247b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from llm_client import HelloAgentsLLM\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. ç¯å¢ƒä¸åº“å¯¼å…¥\n",
    "# ç±»æ¯” C++ çš„ #include ä¸ç¯å¢ƒå˜é‡è®¾ç½®\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "\n",
    "# åˆå§‹åŒ– LLM å®¢æˆ·ç«¯\n",
    "llm_client = HelloAgentsLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df6bfef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. è§„åˆ’å™¨ (Planner) å®šä¹‰ ---\n",
    "# è´Ÿè´£å°†å¤§ç›®æ ‡æ‹†è§£ä¸ºåŸå­ä»»åŠ¡\n",
    "\n",
    "PLANNER_PROMPT_TEMPLATE = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„AIè§„åˆ’ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·æå‡ºçš„å¤æ‚é—®é¢˜åˆ†è§£æˆä¸€ä¸ªç”±å¤šä¸ªç®€å•æ­¥éª¤ç»„æˆçš„è¡ŒåŠ¨è®¡åˆ’ã€‚\n",
    "è¯·ç¡®ä¿è®¡åˆ’ä¸­çš„æ¯ä¸ªæ­¥éª¤éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ã€å¯æ‰§è¡Œçš„å­ä»»åŠ¡ï¼Œå¹¶ä¸”ä¸¥æ ¼æŒ‰ç…§é€»è¾‘é¡ºåºæ’åˆ—ã€‚\n",
    "ä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªPythonåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªæè¿°å­ä»»åŠ¡çš„å­—ç¬¦ä¸²ã€‚\n",
    "\n",
    "é—®é¢˜: {question}\n",
    "\n",
    "è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„è®¡åˆ’ï¼Œ```pythonä¸```ä½œä¸ºå‰åç¼€æ˜¯å¿…è¦çš„:\n",
    "```python\n",
    "[\"æ­¥éª¤1\", \"æ­¥éª¤2\", \"æ­¥éª¤3\", ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class Planner:\n",
    "    def __init__(self, llm_client: HelloAgentsLLM):\n",
    "        self.llm_client = llm_client\n",
    "\n",
    "    def plan(self, question: str) -> list[str]:\n",
    "        prompt = PLANNER_PROMPT_TEMPLATE.format(question=question)\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        print(\"--- æ­£åœ¨ç”Ÿæˆè®¡åˆ’ ---\")\n",
    "        response_text = self.llm_client.think(messages=messages) or \"\"\n",
    "        \n",
    "        # è§£æ LLM è¾“å‡ºçš„ Python åˆ—è¡¨æ¨¡å¼å­—ç¬¦ä¸²\n",
    "        # ç±»æ¯” C++ çš„å­—ç¬¦ä¸²è§£ææˆ–è„šæœ¬è§£é‡Šé€»è¾‘\n",
    "        try:\n",
    "            plan_str = response_text.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "            # ast.literal_eval ç±»æ¯” C++ çš„å®‰å…¨ json/script è§£æï¼Œé¿å…ç›´æ¥æ‰§è¡Œä»£ç \n",
    "            plan = ast.literal_eval(plan_str)\n",
    "            return plan if isinstance(plan, list) else []\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è§£æè®¡åˆ’æ—¶å‡ºé”™: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4fe3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. æ‰§è¡Œå™¨ (Executor) å®šä¹‰ ---\n",
    "# è´Ÿè´£çº¿æ€§æ‰§è¡Œè®¡åˆ’ï¼Œå¹¶ç»´æŠ¤çŠ¶æ€å†å²\n",
    "\n",
    "EXECUTOR_PROMPT_TEMPLATE = \"\"\"\n",
    "ä½ æ˜¯ä¸€ä½é¡¶çº§çš„AIæ‰§è¡Œä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„è®¡åˆ’ï¼Œä¸€æ­¥æ­¥åœ°è§£å†³é—®é¢˜ã€‚\n",
    "ä½ å°†æ”¶åˆ°åŸå§‹é—®é¢˜ã€å®Œæ•´çš„è®¡åˆ’ã€ä»¥åŠåˆ°ç›®å‰ä¸ºæ­¢å·²ç»å®Œæˆçš„æ­¥éª¤å’Œç»“æœã€‚\n",
    "è¯·ä½ ä¸“æ³¨äºè§£å†³â€œå½“å‰æ­¥éª¤â€ï¼Œå¹¶ä»…è¾“å‡ºè¯¥æ­¥éª¤çš„æœ€ç»ˆç­”æ¡ˆï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–å¯¹è¯ã€‚\n",
    "\n",
    "# åŸå§‹é—®é¢˜:\n",
    "{question}\n",
    "\n",
    "# å®Œæ•´è®¡åˆ’:\n",
    "{plan}\n",
    "\n",
    "# å†å²æ­¥éª¤ä¸ç»“æœ:\n",
    "{history}\n",
    "\n",
    "# å½“å‰æ­¥éª¤:\n",
    "{current_step}\n",
    "\n",
    "è¯·ä»…è¾“å‡ºé’ˆå¯¹â€œå½“å‰æ­¥éª¤â€çš„å›ç­”:\n",
    "\"\"\"\n",
    "\n",
    "class Executor:\n",
    "    def __init__(self, llm_client: HelloAgentsLLM):\n",
    "        self.llm_client = llm_client\n",
    "\n",
    "    def execute(self, question: str, plan: list[str]) -> str:\n",
    "        history = \"\"\n",
    "        final_answer = \"\"\n",
    "        \n",
    "        print(\"\\n--- æ­£åœ¨æ‰§è¡Œè®¡åˆ’ ---\")\n",
    "        for i, step in enumerate(plan, 1):\n",
    "            print(f\"\\n-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ {i}/{len(plan)}: {step}\")\n",
    "            prompt = EXECUTOR_PROMPT_TEMPLATE.format(\n",
    "                question=question, plan=plan, history=history if history else \"æ— \", current_step=step\n",
    "            )\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            \n",
    "            response_text = self.llm_client.think(messages=messages) or \"\"\n",
    "            \n",
    "            # ç»´æŠ¤å†å²è®°å½•ï¼ˆState Managementï¼‰ï¼Œç±»æ¯” C++ çš„ä¸Šä¸‹æ–‡çŠ¶æ€æœºæ›´æ–°\n",
    "            # å°†æ¯ä¸€æ­¥çš„ç»“æœå­˜å…¥ historyï¼Œä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥\n",
    "            history += f\"æ­¥éª¤ {i}: {step}\\nç»“æœ: {response_text}\\n\\n\"\n",
    "            final_answer = response_text\n",
    "            \n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354b60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­£åœ¨ç”Ÿæˆè®¡åˆ’ ---\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "```python\n",
      "[\"æ­¥éª¤1: ç¡®å®šå‘¨ä¸€å–å‡ºçš„è‹¹æœæ•°é‡ä¸º15ä¸ªã€‚\",\n",
      " \"æ­¥éª¤2: è®¡ç®—å‘¨äºŒå–å‡ºçš„è‹¹æœæ•°é‡ï¼Œç­‰äºå‘¨ä¸€çš„ä¸¤å€ï¼Œå³15 * 2ã€‚\",\n",
      " \"æ­¥éª¤3: è®¡ç®—å‘¨ä¸‰å–å‡ºçš„è‹¹æœæ•°é‡ï¼Œæ¯”å‘¨äºŒå°‘5ä¸ªï¼Œå³å‘¨äºŒçš„æ•°é‡ - 5ã€‚\",\n",
      " \"æ­¥éª¤4: å°†å‘¨ä¸€ã€å‘¨äºŒå’Œå‘¨ä¸‰å–å‡ºçš„è‹¹æœæ•°é‡ç›¸åŠ ï¼Œå¾—åˆ°æ€»æ•°ã€‚\",\n",
      " \"æ­¥éª¤5: è¾“å‡ºä¸‰å¤©æ€»å…±å–å‡ºçš„è‹¹æœæ•°é‡ã€‚\"]\n",
      "```\n",
      "\n",
      "--- æ­£åœ¨æ‰§è¡Œè®¡åˆ’ ---\n",
      "\n",
      "-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ 1/5: æ­¥éª¤1: ç¡®å®šå‘¨ä¸€å–å‡ºçš„è‹¹æœæ•°é‡ä¸º15ä¸ªã€‚\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "15\n",
      "\n",
      "-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ 2/5: æ­¥éª¤2: è®¡ç®—å‘¨äºŒå–å‡ºçš„è‹¹æœæ•°é‡ï¼Œç­‰äºå‘¨ä¸€çš„ä¸¤å€ï¼Œå³15 * 2ã€‚\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "30\n",
      "\n",
      "-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ 3/5: æ­¥éª¤3: è®¡ç®—å‘¨ä¸‰å–å‡ºçš„è‹¹æœæ•°é‡ï¼Œæ¯”å‘¨äºŒå°‘5ä¸ªï¼Œå³å‘¨äºŒçš„æ•°é‡ - 5ã€‚\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "25\n",
      "\n",
      "-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ 4/5: æ­¥éª¤4: å°†å‘¨ä¸€ã€å‘¨äºŒå’Œå‘¨ä¸‰å–å‡ºçš„è‹¹æœæ•°é‡ç›¸åŠ ï¼Œå¾—åˆ°æ€»æ•°ã€‚\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "70\n",
      "\n",
      "-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ 5/5: æ­¥éª¤5: è¾“å‡ºä¸‰å¤©æ€»å…±å–å‡ºçš„è‹¹æœæ•°é‡ã€‚\n",
      "ğŸ§  æ­£åœ¨è°ƒç”¨ gpt-4o æ¨¡å‹...\n",
      "âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:\n",
      "70\n",
      "\n",
      "--- ä»»åŠ¡å®Œæˆ ---\n",
      "æœ€ç»ˆç­”æ¡ˆ: 70\n"
     ]
    }
   ],
   "source": [
    "# --- 4. æ™ºèƒ½ä½“ (Agent) æ•´åˆ ---\n",
    "\n",
    "class PlanAndSolveAgent:\n",
    "    def __init__(self, llm_client: HelloAgentsLLM):\n",
    "        self.llm_client = llm_client\n",
    "        self.planner = Planner(self.llm_client)\n",
    "        self.executor = Executor(self.llm_client)\n",
    "\n",
    "    def run(self, question: str):\n",
    "        # ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’ (Compiler Frontend)\n",
    "        plan = self.planner.plan(question)\n",
    "        if not plan:\n",
    "            print(\"æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„è¡ŒåŠ¨è®¡åˆ’ã€‚\")\n",
    "            return\n",
    "            \n",
    "        # ç¬¬äºŒé˜¶æ®µï¼šæ‰§è¡Œ (Compiler Backend)\n",
    "        final_answer = self.executor.execute(question, plan)\n",
    "        print(f\"\\n--- ä»»åŠ¡å®Œæˆ ---\\næœ€ç»ˆç­”æ¡ˆ: {final_answer}\")\n",
    "\n",
    "# --- è¿è¡Œæµ‹è¯• ---\n",
    "agent = PlanAndSolveAgent(llm_client)\n",
    "# è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„éœ€è¦å¤šæ­¥é€»è¾‘æ¨ç†çš„æ•°å­¦é¢˜\n",
    "question = \"ä¸€ä¸ªæ°´æœåº—å‘¨ä¸€å–å‡ºäº†15ä¸ªè‹¹æœã€‚å‘¨äºŒå–å‡ºçš„è‹¹æœæ•°é‡æ˜¯å‘¨ä¸€çš„ä¸¤å€ã€‚å‘¨ä¸‰å–å‡ºçš„æ•°é‡æ¯”å‘¨äºŒå°‘äº†5ä¸ªã€‚è¯·é—®è¿™ä¸‰å¤©æ€»å…±å–å‡ºäº†å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\"\n",
    "agent.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
